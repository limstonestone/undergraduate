X = Data1.1.1[-1]
rownames(X) = Data1.1.1[, 1]
X
Data1.1.1
### CODE 2.2.1
# PCA 수행단계
Data1.1.1 = read.table("3subjects.txt", header=T)
X = Data1.1.1[-1]
X
# [Step 2] Covariance Matrix S(or Correlation Matrix R)
S = round(cov(X), 3)
S
# [Step 3] Spectral Decomposition
eigen.S = eigen(S)
round(eigen.S$values, 3)  # Eigenvalues
V = round(eigen.S$vectors, 3) # Eigenvectors
V
# [Step 4] Choice of Eigenvalues and Eigenvectors
gof = eigen.S$values/sum(eigen.S$values)*100  # Goodness-of fit
round(gof, 2)
# [Step 5] PCs : linear combination of original variables
V2 = V[,1:2]
V2
Y
# [Step 6] PCS, PCs Scores and New Data Matrix P
Y = scale(X, scale=F) # Centered Data Matrix
Y
P
P = Y%*%V2 # PCs Scores
P
# [Step 7] Plot of PCs Scores
par(pty="s")
lim = range(pretty(P))
plot(P[, 1], P[, 2], main="Plot of PCs Scores", xlim=lim, ylim=lim,
xlab="1st PC", ylab="2nd PC")
pretty(P)
P
text(P[,1]+2, P[, 2], rownames(X), cex=0.8, col="blue", pos=3)
abline(v=0, h=0)
# Steps for PCA
X = read.table("5subjects.txt", header=T)
head(X0)
head(X)
X = X[, -1]
dim(X)
#[Step 2] Covariance Matrix S(or Correlation Matix R)
S=round(cov(X),3)
S
#[Step 3] Spectral Decomposition (SD)
eigen.S=eigen(S)
round(eigen.S$values, 3) # Eigenvalues
V=round(eigen.S$vectors, 3) # Eigenvectors
V
#[Step 4] Choice of Eigenvalues and Eigenvectors
gof=eigen.S$values/sum(eigen.S$values)*100 # Goodness-of fit
round(gof, 2)
plot(eigen.S$values, type="b", main="Scree Graph", xlab="Component Number", ylab="Eigenvalue")
#[Step 5] PCs : liner combination of original variables
V2=V[,1:2]
V2
#[Step 6] PCS, PCs Scores and New Data Matrix P
Y=scale(X, scale=F) # Centred Data Matrix (Covariance<Centred>, Correlation<Standardized>)
P=Y%*%V2            # PCs Scores
head(P)
rownames(P) <- 1:88
#[Step 7] Plot of PCs Scores
plot(P[,1], P[, 2], main="Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P[,1], P[, 2]+2, labels=rownames(P), cex=0.8, col="blue")
abline(v=0, h=0)
# [Step 1] Input Data
S1 = matrix(c(80, 44, 44, 80), byrow=T, nrow=2)
S2 = matrix(c(8000, 440, 440, 80), byrow=T, nrow=2)
# [Step 2] Covariance Matrix S(or Correlation Matrix R)
S = S1 # S=S2
S
# [Step 3] Spectral Decomposition
eigen.S = eigne(S)
round(eigen.S$values, 3)  # Eigenvalues
V = round(eigen.S$vectors, 3) # Eigenvectors
V
# [Step 4] Choice of Eigenvalues and Eigenvecotrs
gof = eigen.S$values/sum(eigen.S$values)*100  # Goodness-of-fit
round(gof, 2)
# [Step 5] PCs : linear combination of original variables
V2 = V[, 1:2]
V2
# Correlations bt PCs and variables
Ds = diag(diag(1/sqrt(S)))
D = diag(sqrte(eigen.S$values))
corr = Ds %*% V2 %*% D
corr = Ds%*%V2 %*% D
# Correlations bt PCs and variables
Ds = diag(diag(1/sqrt(S)))
D = diag(sqrte(eigen.S$values))
corr = Ds %*% V2 %*% D
corr = Ds%*%V2%*%D
Ds
# Correlations bt PCs and variables
Ds = diag(diag(1/sqrt(S)))
D = diag(sqrt(eigen.S$values))
corr = Ds%*%V2%*%D
# [Step 5] PCs : linear combination of original variables
V2 = V[, 1:2]
V2
corr = Ds%*%V2%*%D
V2
D
corr = Ds%*%V2.T%*%D
corr = Ds%*%t(V2)%*%D
corr
V2 &*& t(V2)
V2 %*% t(V2)
V2
V2
S
# [Step 3] Spectral Decomposition
eigen.S = eigne(S)
round(eigen.S$values, 3)  # Eigenvalues
V = round(eigen.S$vectors, 3) # Eigenvectors
V
# [Step 1] Input Data
S1 = matrix(c(80, 44, 44, 80), byrow=T, nrow=2)
S2 = matrix(c(8000, 440, 440, 80), byrow=T, nrow=2)
# [Step 2] Covariance Matrix S(or Correlation Matrix R)
S = S1 # S=S2
S
# [Step 3] Spectral Decomposition
eigen.S = eigen(S)
round(eigen.S$values, 3)  # Eigenvalues
V = round(eigen.S$vectors, 3) # Eigenvectors
V
# [Step 4] Choice of Eigenvalues and Eigenvecotrs
gof = eigen.S$values/sum(eigen.S$values)*100  # Goodness-of-fit
round(gof, 2) # choice sum(gof) > 70
# [Step 5] PCs : linear combination of original variables
V2 = V[, 1:2]
V2
# Correlations bt PCs and variables
Ds = diag(diag(1/sqrt(S)))
D = diag(sqrt(eigen.S$values))
corr = Ds %*% V2 %*% D
corr
V2 %*% t(V2)
t(V2) %*% V2
plot(eigen.S$values, type="b", main="Scree Graph", xlab="Component Number", ylab="Eigenvalue")
### CODE 2.4.1
# PCA steps and correlation coefficients bt PCs and variables
# [Step 1] Input Data
S1 = matrix(c(80, 44, 44, 80), byrow=T, nrow=2)
S2 = matrix(c(8000, 440, 440, 80), byrow=T, nrow=2)
# [Step 2] Covariance Matrix S(or Correlation Matrix R)
S = S1 # S=S2
S
# [Step 3] Spectral Decomposition
eigen.S = eigen(S)
round(eigen.S$values, 3)  # Eigenvalues
V = round(eigen.S$vectors, 3) # Eigenvectors
V
# [Step 4] Choice of Eigenvalues and Eigenvecotrs
gof = eigen.S$values/sum(eigen.S$values)*100  # Goodness-of-fit
round(gof, 2) # choice sum(gof) > 70
plot(eigen.S$values, type="b", main="Scree Graph", xlab="Component Number", ylab="Eigenvalue")
# [Step 5] PCs : linear combination of original variables
V2 = V[, 1:2]
V2
# Correlations bt PCs and variables
Ds = diag(diag(1/sqrt(S)))
D = diag(sqrt(eigen.S$values))
corr = Ds %*% V2 %*% D
corr
### CODE 2.4.1
# PCA steps and correlation coefficients bt PCs and variables
# [Step 1] Input Data
S1 = matrix(c(80, 44, 44, 80), byrow=T, nrow=2)
S2 = matrix(c(8000, 440, 440, 80), byrow=T, nrow=2)
# [Step 2] Covariance Matrix S(or Correlation Matrix R)
S = S1 # S=S2
S
# [Step 3] Spectral Decomposition
eigen.S = eigen(S)
round(eigen.S$values, 3)  # Eigenvalues
V = round(eigen.S$vectors, 3) # Eigenvectors
V
# [Step 4] Choice of Eigenvalues and Eigenvecotrs
gof = eigen.S$values/sum(eigen.S$values)*100  # Goodness-of-fit
round(gof, 2) # choice sum(gof) > 70
plot(eigen.S$values, type="b", main="Scree Graph", xlab="Component Number", ylab="Eigenvalue")
# [Step 5] PCs : linear combination of original variables
V2 = V[, 1:2]
V2
# Correlations bt PCs and variables
Ds = diag(diag(1/sqrt(S)))
D = diag(sqrt(eigen.S$values))
corr = Ds %*% V2 %*% D
corr
rm(list=ls())
### CODE 2.4.1
# PCA steps and correlation coefficients bt PCs and variables
# [Step 1] Input Data
S1 = matrix(c(80, 44, 44, 80), byrow=T, nrow=2)
S2 = matrix(c(8000, 440, 440, 80), byrow=T, nrow=2)
# [Step 2] Covariance Matrix S(or Correlation Matrix R)
S = S1 # S=S2
S
# [Step 3] Spectral Decomposition
eigen.S = eigen(S)
round(eigen.S$values, 3)  # Eigenvalues
V = round(eigen.S$vectors, 3) # Eigenvectors
V
# [Step 4] Choice of Eigenvalues and Eigenvecotrs
gof = eigen.S$values/sum(eigen.S$values)*100  # Goodness-of-fit
round(gof, 2) # choice sum(gof) > 70
plot(eigen.S$values, type="b", main="Scree Graph", xlab="Component Number", ylab="Eigenvalue")
# [Step 5] PCs : linear combination of original variables
V2 = V[, 1:2]
V2
# Correlations bt PCs and variables
Ds = diag(diag(1/sqrt(S)))
D = diag(sqrt(eigen.S$values))
corr = Ds %*% V2 %*% D
corr
# [Step 1] Data Matrix X
Data1.3.2 = read.table("klpga.txt", header=T)
# [Step 1] Data Matrix X
Data1.3.2 = read.table("klpga.txt", header=T, fileEncoding="euc-kr")
X = Data1.3.2
rownames=  rownames(X)
# [Step 2] Covariance Matrix S(or Correlaiton Matrix R)
R = round(cor(X), 3)
R
# [Step 3] Spectral Decomposition
eigen.R = eigen(R)
round(eigen.R$values, 2)  # Eigenvalues
V = round(eigen.R$vectors, 2) # Eigenvectors
vec = eigen.R$vectors
val = diag(eigen.R$values)
vec %*% val %*% t(vec)
# [Step 4] Choice of Eigenvalues and Eigenvectors
gof = eigen.R$values/sum(eigen.R$values)*100 # Goodness-of fit
round(gof, 2)
par(mfrow=c(1,1))
plot(eigen.R$values, type="b", main="Scree Graph", xlab="Component Number", ylab="Eigenvalue")
# [Step 5] PCs : linear combination of original variables
V2 = V[, 1:2]
V2
# [Step 6] PCS, PCs Scores and New Data Matrix P
Z = scale(X, scale=T) # Stnadardized Data Matrix
head(Z)
P = Z %*% V2 # PCs Scores
head(round(P, 3))
# [Step 7] Plot of PCs Scores
plot(P[,1], P[, 2], main="Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P[,1], P[, 2], labels=rownames, cex=0.8, col="blue", pos=3)
abline(v=0, h=0)
# Correlation between PCs and variables
D = diag(sqrt(eigen.R$values[1:2]))
corr = V2 %*% D
corr
colnames(corr) = c("gamma1", "gamma2")
rownames(corr) = colnames(X)
corr
# [Step 1] Data Matrix X(standized data)
Data1.3.2 = read.table("skull.txt", header=T)
Z = as.matrix(Data1.3.2)
rownames = rownames(Z)
colnames = colnames(Z)
n = nrow(Z)
# [Step 2] Covariance Matrix S(or Correlation Matrix R)
R = (t(Z) %*% Z/(n-1))
R
cov(Z)
R # =cov(Z)
# [Step 3] Spectral Decomposition
eigen.R = eigen(R)
round(eigen.R$values, 2)  # Eigenvalues
V = eigen.R$vectors # Eigenvectors
round(V, 2)
# [Step 4] Choice of Eigenvalues and Eigenvectors
gof = eigen.R$values/sum(eigen.R$values)*100  # Goodness-of fit
round(gof, 2)
plot(eigen.R$values, type="b", main="Scree Graph", xlab="Component Number", ylab="Eigenvalue")
# [Step 5] PCs : linear combination of original variables
V3 = V[, 1:3]
round(t(V3), 2)
# [Step 6] PCS, PCs Scores and New Data Matrix P
Z # Standardized Data Matrix
P = Z %*% V3 # PCs Scores
head(round(P, 3))
# [Step 7] Plot of PCs Scores
par(mfrow=c(2,2))
plot(P[,1], P[, 2], main="(a) Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P[,1], P[, 2], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
plot(P[,1], P[, 3], main="(b) Plot of PCs Scores", xlab="1st PC", ylab="3rd PC")
text(P[,1], P[, 3], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
plot(P[,2], P[, 3], main="(c) Plot of PCs Scores", xlab="2nd PC", ylab="3rd PC")
text(P[,2], P[, 3], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
substr(x, nchar(x) - n+1, nchar(x))
# Classifying Man and Woman using different pixel color
substr_Right = function(x, n){
substr(x, nchar(x) - n+1, nchar(x))
}
levels = substr_Right(rownames, 1)
levels = as.factor(levels)
plot(P[,1], P[, 2], main="(a) Plot of PCs Scores", xlab="1st PC", ylab="2nd PC", col=levels, pch=16)
text(P[,1], P[, 2], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
levels
# [Step 1] Data Matrix X
Data1.3.2. = read.table("skull.txt", header=T)
Z = as.matrix(Data1.3.2)
rownames = rownames(Z)
colnames = colnames(Z)
n = nrow(z)
n = nrow(Z)
# [Step 2] Singular Values Decompoisiton
svd.Z = svd(Z)
U = svd.Z$u # Right singular vectors
V = svd.Z$v # Left singhular vectors : Eigenvectors
round(V, 2)
D = diag(svd.Z$d)
# [Step 3] Choice of Singular Values and Eigenvectors
round(svd.Z$d, 2)
eigen = (svd.Z$d)^2
round(eigen/(n-1), 2)
gof = eigen/sum(eigen)*100  # Goodness-of-fit
round(gof, 2)
# [Step 5] PCs : linear combination of original variables
V3 = V[, 1:3]
V3
round(t(V3), 2)
# [Step 6] PCS, PCs Scores and New Data Matrix P
Z # Stnadardized Data Matrix
P = U %*% D # PCs Scores : P = Z %*% V3
round(P, 3)
plot(P[,1], P[, 2], main="(a) Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P[,1], P[, 2], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
plot(P[,1], P[, 3], main="(b) Plot of PCs Scores", xlab="1st PC", ylab="3rd PC")
text(P[,1], P[, 3], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
plot(P[,2], P[, 3], main="(c) Plot of PCs Scores", xlab="2nd PC", ylab="3rd PC")
text(P[,2], P[, 3], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
#Correlations bt PCs and variables
D=diag(svd.Z$d[1:3]/sqrt(n-1))
corr=V3%*%D
round(corr, 3)
### CODE 2.6.1
# PC Biplots for 5 Subjects Exam
X = read.table("5subjects.txt", header=T)
n = nrow(X)
rownames(X)
colnames(x)
colnames(X)
X = X[, -1]
joinnames = c(rownames(X), colnames(X))
Y = scale(X, scale=F)
# Biplot based on the Singular Value Decomposition
svd.Y = svd(Y)
U = svd.Y$u
V = svd.Y$v
D = diag(svd.Y$d)
C = rbind(G, H)
H = (sqrt(1/(n-1))*V %*% D)[, 1:2]  # 변수
C = rbind(G, H)
G = (sqrt(n-1)*U)[, 1:2]  # 표본
H = (sqrt(1/(n-1))*V %*% D)[, 1:2]  # 변수
C = rbind(G, H)
rownames(G) = rownames(X)
rownames(H) = colnames(X)
rownames(C) = joinnames
C
# Goodness-of-fit
eig = (svd.Y$d)^2
per = eig/sum(eig)*100
gof = sum(per[1:2])
# Biplots
par(mfrow=c(2,2))
par(pty="s")
lim1 = range(pretty(H))
plot(H[,1],H[,2],xlab="1st PC",ylab="2nd PC", main="(a) 5 Subjects",
xlim=lim1,ylim=lim1,pch=15,col=2, type="n")
abline(v=0,h=0)
text(H[,1], H[,2],colnames(X),cex=0.8,col=1,pos=3)
arrows(0,0,H[,1],H[,2],col=2,code=2, length=0.1)
lim2 <- range(pretty(G))
plot(G[,1],G[,2],xlab="1st PC",ylab="2nd PC", main="(b) 88 Students",
xlim=lim2,ylim=lim2,pch=16, type="n")
abline(v=0,h=0)
text(G[,1],G[,2],rownames(X),cex=0.8,pos=3)
lim3 <- range(pretty(C))
plot(C[,1],C[,2],xlab="1st PC",ylab="2nd PC",  main="(c) 5 Subjects and 88 Students",
xlim=lim3,ylim=lim3,pch=16,  type="n")
abline(v=0,h=0)
text(C[,1],C[,2],joinnames,cex=0.8,pos=3)
arrows(0,0,C[89:93,1],C[89:93,2],col=2,code=2, length=0.1)
biplot(G,H, xlab="1st PC",ylab="2nd PC", main="(d) biplot function",
xlim=lim2,ylim=lim2,cex=0.8,pch=16)
abline(v=0,h=0)
par(mfrow=c(2,2))
par(pty="s")
lim1 = range(pretty(H))
plot(H[,1],H[,2],xlab="1st PC",ylab="2nd PC", main="(a) 5 Subjects",
xlim=lim1,ylim=lim1,pch=15,col=2, type="n")
abline(v=0,h=0)
text(H[,1], H[,2],colnames(X),cex=0.8,col=1,pos=3)
arrows(0,0,H[,1],H[,2],col=2,code=2, length=0.1)
lim2 <- range(pretty(G))
plot(G[,1],G[,2],xlab="1st PC",ylab="2nd PC", main="(b) 88 Students",
xlim=lim2,ylim=lim2,pch=16, type="n")
abline(v=0,h=0)
text(G[,1],G[,2],rownames(X),cex=0.8,pos=3)
lim3 <- range(pretty(C))
plot(C[,1],C[,2],xlab="1st PC",ylab="2nd PC",  main="(c) 5 Subjects and 88 Students",
xlim=lim3,ylim=lim3,pch=16,  type="n")
abline(v=0,h=0)
text(C[,1],C[,2],joinnames,cex=0.8,pos=3)
arrows(0,0,C[89:93,1],C[89:93,2],col=2,code=2, length=0.1)
biplot(G,H, xlab="1st PC",ylab="2nd PC", main="(d) biplot function",
xlim=lim2,ylim=lim2,cex=0.8,pch=16)
abline(v=0,h=0)
biplot(G,H, xlab="1st PC",ylab="2nd PC", main="(d) biplot function",
xlim=lim2,ylim=lim2,cex=0.8,pch=16)
biplot(G,H, xlab="1st PC",ylab="2nd PC", main="(d) biplot function",
xlim=lim2,ylim=lim2,cex=0.8,pch=16)
abline(v=0,h=0)
biplot(G,H, xlab="1st PC",ylab="2nd PC", main="(d) biplot function",
xlim=lim2,ylim=lim2,cex=0.8,pch=16)
biplot(G,H, xlab="1st PC",ylab="2nd PC", main="(d) biplot function",
xlim=lim2,ylim=lim2,cex=0.8,pch=16)
abline(v=0,h=0)
### CODE 2.6.2
# PC Biplot for KLPGA
# Data Matrix X
Data1.3.2 = read.table("klpga.txt", header=T)
### CODE 2.6.2
# PC Biplot for KLPGA
# Data Matrix X
Data1.3.2 = read.table("klpga.txt", header=T, fileEncoding="euc-k3")
### CODE 2.6.2
# PC Biplot for KLPGA
# Data Matrix X
Data1.3.2 = read.table("klpga.txt", header=T, fileEncoding="euc-kr")
X = Data1.3.2
n = nrow(X)
rownames(X)
colnames(X)
Y = scale(X, scale=T)
# Biplot based on the Singular Value Decomposition
svd.Y = svd(Y)
U = svd.Y$u
V = svd.Y$v
D = diag(svd.Y$d)
G = (sqrt(n-1)*U)[, 1:2]
H = (sqrt(1/(n-1))*V %*% D)[, 1:2]
rownames(G) = rownames(X)
rownames(H) = colnames(X)
# Goodness-of-fit
eig = (svd.Y$d)^2
per = eig/sum(eig)*100
gof = sum(per[1:2])
round(per, 2)
round(gof, 2)
# PC Biplot
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot for KLPGA Data ",
xlim=lim,ylim=lim,cex=0.8,pch=16)
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot for KLPGA Data ",
xlim=lim,ylim=lim,cex=0.8,pch=16)
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot for KLPGA Data ",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
# PC Biplot
par(mfrow=c(1,1))
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot for KLPGA Data ",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
### CODE 2.7.1
# Easy way to use PCA -> princomp() or prcomp()
# Example for KLPGA
Data1.3.2 = read.table("klpga.txt", header=T)
### CODE 2.7.1
# Easy way to use PCA -> princomp() or prcomp()
# Example for KLPGA
Data1.3.2 = read.table("klpga.txt", header=T, fileEncoding="euc-kr")
X = Data1.3.2
# PCA based on the Sd using princomp()
pca.R = princomp(X, cor=T)  # cor=F -> covariance, cor=T -> correlation
summary(pca.R, loadings=T)  # 설명력, 주성분계수
pca.R
round(pca.R$scores, 3)  # 주성분점수
par(mfrow=c(1,1))
screeplot(pca.R, type="lines")  # Scree Plot
# Principle component biplot(SD)
biplot(pca.R, scale=0, xlab="1st PC", ylab="2nd PC",
main="PC Biplot for KLPGA Data")
abline(v=0, h=0)
# PCA on the SVD using prcomp()
pcasvd.Z = prcomp(X, scale=T)
summary(pcasvd.Z) # 설명력
summary(pcasvd.Z) # explanation
round(pcasvd.Z$rotation, 3) # PC coefficient
pcasvd.Z$scale
screeplot(pcasvd.Z, type="lines")
# Principle component biplot (SVD)
biplot(pcasvd.Z, scale=0,  xlab="1st PC", ylab="2nd PC",
main="PC Biplot for KLPGA Data")
abline(v=0, h=0)
