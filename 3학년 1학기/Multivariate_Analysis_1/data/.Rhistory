SSB
tapply(y, factor, mean)
groupMeans
pf(f.value, g-1, N-g, lower.tail=FALSE) # lower.tail : calculate only tail
pf(f.value, g-1, N-g, lower.tail=FALSE) # lower.tail : calculate only tail
SSW = 0
for (j in 1:g) {
SSW = SSW + sum((y[factor==j] - groupMeans[j])^2)
}
g = 3
n = 4
N = 12
ssb = 0
for (j in 1:g) {
ssb = ssb + (groupMeans[j] - mean(y))^2
}
SSB = n * ssb
SSB
SSW = 0
for (j in 1:g) {
SSW = SSW + sum((y[factor==j] - groupMeans[j])^2)
}
f.value = (SSB/(g-1)) / (SSW/(N-g))
f.value
pf(f.value, g-1, N-g, lower.tail=FALSE) # lower.tail : calculate only tail
pf(f.value, g-1, N-g, lower.tail=TRUE)
g = lm(y ~ factor)
anova(g)
summary(aov(g))
SSW
SSB
# Linear Model in ANOVA
set.seed(123)
y = c(rnorm(10, -1, 1), rnorm(10, 0, 1), rnorm(10, 1, 1))
y
factor = gl(3, 10)
factor
model.matrix(y ~ factor -1)
model.matrix(y ~ factor)
g1 = lm(y ~ factor -1)
g2 = lm(y ~ factor)
summary(g1)$coef
summary(g2)$coef
anova(g1)
anova(g2)
anova(g1) # Wrong result (we can see df=2)
anova(g2)
summary(g1)$sigma
summary(g2)$sigma
summary(g1)
summary(g1)$sigma
cbind(residuals(g1), residuals(g2))
cbind(fitted(g1), fitted(g2))
tapply(y, factor, mean)
factor = relevel(factor, ref=2)
facotr
factor
model.matirx(y ~ factor)
model.matrix(y ~ factor)
g3 = lm(y ~ factor)
summary(g2)$coef
summary(g3)$coef
anova(g2)
anova(g3)
factor = gl(3, 10)
summary(lm(y ~ factor))
anova(lm(y ~ factor))
# All Pairwise Test
## By above, the difference between Group2 and 3 is not tested
## pairwise.t.test() : all possible pairwise t-tests
pairwise.t.test(y, factor, p.adjust.method="bonferroni")
pairwise.t.test(y, factor, p.adjust.method="hom")
pairwise.t.test(y, factor, p.adjust.method="holm")
TukeyHSD(aov(y ~ factor))
## A popular multiple comparison test for post-hoc ANOVA is Tukey's HSD test
TukeyHSD(aov(y ~ factor))
y
plot(TukeyHSD(aov(y ~ factor)))
## Example : No difference between the means
set.seed(1234)
y = rnorm(30, 1.9, 1)
y
factor = gl(3, 10)
g = lm(y ~ factor)
anova(g)
pairwise.t.test(y, factor, p.adjust.method="bonferroni")
TukeyHSD(aov(g))
plot(TukeyHSD(aov(g)))
BiocManager::install("ALL")
library(ALL)
data(ALL)
?ALL
dim(ALL)
ALL[1:10, 1:5]
str(ALL)
exprs(ALL)[1:10, 1:5]
dim(exprs(ALL))
table(ALL$BT)
B1B2B3 = ALL$BT %in% c("B1", "B2", "B3")
ex = exprs(ALL)
View(X)
View(ex)
y = as.numeric(ex[row.names(ex)=="1866_g_at", B1B2B3])
factor = factor(ALL$BT[B1B2B3], labels=c("B1", "B2", "B3"))
col = c("orange", "darkgreen", "blue")
xlab = "B-cell ALL stage"
ylab = "SKT-like oncogene expression"
par(mfrow=c(1, 2))
stripchart(y ~ factor, method="jitter", cex.lab=1.5, xlab=xlab,
col=col, vertical=TRUE, ylab=ylab)
boxplot(y ~ factor, cex.lab=1.5, main=NULL, boxwex=0.3, col=col,
xlab=xlab, ylab=ylab)
## We test experimental effects
g = lm(y ~ factor)
summary(g)
anova(g)
summary(aov(g))
pairwise.t.test(y, factor, p.adjust.method="bonferroni")
TukeyHSD(aov(g))
par(mfrow=c(1,1))
plot(TukeyHSD(aov(g)))
## other example
y2 = as.numeric(ex[row.names(ex)=="1242_at", B1B2B3])
ylab = "Ets2 expression"
par(mfrow=c(1, 2))
par(mfrow=c(1, 2))
stripchart(y2 ~ factor, method="jitter", cex.lab=1.5,
vertical=TRUE, xlab=xlab, ylab=ylab, col=col)
boxplot(y2 ~ factor, cex.lab=1.5, main=NULL, xlab=xlab,
boxwex=0.3, ylab=ylab, col=col)
g = lm(y2 ~ factor)
summary(g)
anova(g)
summary(aov(g))
pairwise.t.test(y2, factor, p.adjust.method="bonferroni")
TukeyHSD(aov(g))
plot(TukeyHSD(aov(g)))
par(mfrow=c(1,1))
plot(TukeyHSD(aov(g)))
## other example (How many genes of the ALL data is H0 of equal means rejected?)
dim(ex[, B1B2B3])
fun = function(t) anova(lm(t ~ factor))$Pr[1]
anova.pValues = apply(ex[, B1B2B3], 1, fun)
pBF = p.adjust(anova.pValues, method="bonferroni")
pHO = p.adjust(anova.pValues, method="holm")
pBH = p.adjust(anova.pValues, method="BH")
alpha = 0.05
c(sum(pBF < alpha), sum(pHO < alpha), sum(pBH < alpha))
# Checking Assumptions
library(ALL)
data(ALL)
B1B2B3 = ALL$BT %in% c("B1", "B2", "B3")
ex = exprs(ALL)
y = as.numeric(ex[row.names(ex)=="1866)g)at", B1B2B3])
factor = factor(ALL$BT[B1B2B3], labels=c("B1", "B2", "B3"))
res = residuals(lm(y ~ factor))
shapiro.test(res)
res = residuals(lm(y ~ factor))
B1B2B3 = ALL$BT %in% c("B1", "B2", "B3")
ex = exprs(ALL)
y = as.numeric(ex[row.names(ex)=="1866_g_at", B1B2B3])
factor = factor(ALL$BT[B1B2B3], labels=c("B1", "B2", "B3"))
res = residuals(lm(y ~ factor))
shapiro.test(res)
par(mfrow=c(1, 2))
qqnorm(res, pch=19, cex.lab=1.5, col="red", main=NULL)
qqline(res)
hist(res, nclass=20, col="orange", xlab="residuals", main="")
install.packages("lmtest")
library(lmtest)
bptest(lm(y ~ factor), studentize=FALSE)
bartlett.test(y ~ factor)
filgner.test(y, factor)
fligner.test(y, factor)
library(car)
leveneTest(y, factor)
fit = lm(y ~ factor)$fit
plot(fit, res, pch=19, col="red", xlab="Fitted values", ylab="Residuals")
par(mfrow=c(1,1))
plot(fit, res, pch=19, col="red", xlab="Fitted values", ylab="Residuals")
## Example (Unequal variances)
oneway.test(y ~ factor, var.equal=FALSE)
# Least Square Estimation
data(golub, package="multtest")
zyxin = grep("Zyxin", golub.gnames[,2], ignore.case=TRUE)
cmyb = grep("c-myb", golub.gnames[,2], ignore.case=TRUE)
x = golub[zyxin, ]
y = golub[cmyb, ]
leastSquares = lm(y ~ x)
leastSquares$coef
plot(x, y, pch=19, xlab="Relative zyxin gene expression", ylab="Relative c-MYB gene expression", cex.lab=1.5, col="blue")
abline(leastSquares$coef, lwd=3, lty=2, col="red")
lmSummary = summary(leastSquares)
lmSummary
## Signif.codes : more stars represents more significance
## R^2 : indicates how well the data points fit the statistical model
## (percentage of variance in the dependent variable that is explained by the model)
## adjusted R-sqaured : more predictors -> improve R-squared. adjusted R-sqaured can adjust this unreasonable issue.
# F Test Statistic
## F-value means "better than random"
## F-value is the ratio of the variance explained over the unexplained variance (residual)
f.value = (coef(lmSummary)[2, "t value"])^2 # In the simple case -> F = t^2
f.value
lmSummary$fstat
# Linear Model with a Factor
y = c(2, 3, 1, 2, 8, 7, 9, 8, 11, 12, 13, 12)
factor = gl(3, 4) # gl : Generate factors by specifying the pattern of their levels.
factor
model.matrix(y ~ factor - 1)  # model.matrix = design matrix X, -1 indicates model has no intercept
## estimation of the coefficients = estimates of the population group means from the sample data
summary(lm(y ~ factor -1))  # p-values are too small -> Reject H0 (mu_j = 0)
## linear model is useful for testing hypotheses about group means
# One-way Analysis of Variance
## ANOVA can perform for testing three or more population means are equal to each other
## SSW(sum of squares within) is some of the squared deviation of the measurements to their group mean
## SSB(sum of squares betwwen) is the sum of sqaures the deviances of the group mean w.r.t total mean
## f-value is defined by {SSB/(g-1)}/{SSW/(N-g)}
## if F-value > f => H0(mu_1 = mu_2 = mu_3 ,,, ) is rejected
y = c(2, 3, 1, 2, 8, 7, 9, 8, 11, 12, 13, 12)
factor = gl(3, 4)
groupMeans = as.numeric(tapply(y, factor, mean))  # tapply : perform function about factor
groupMeans
mean(y)
g = 3
n = 4
N = 12
ssb = 0
for (j in 1:g) {
ssb = ssb + (groupMeans[j] - mean(y))^2
}
SSB = n * ssb
SSB
SSW = 0
for (j in 1:g) {
SSW = SSW + sum((y[factor==j] - groupMeans[j])^2)
}
f.value = (SSB/(g-1)) / (SSW/(N-g))
f.value
pf(f.value, g-1, N-g, lower.tail=FALSE) # lower.tail : calculate only tail
g = lm(y ~ factor)
anova(g)
summary(aov(g))
## fator : SSB
## residuals : SSW
# Linear Model in ANOVA
set.seed(123)
y = c(rnorm(10, -1, 1), rnorm(10, 0, 1), rnorm(10, 1, 1))
y
factor = gl(3, 10)
factor
model.matrix(y ~ factor -1)
model.matrix(y ~ factor)  # Reference : factor1
g1 = lm(y ~ factor -1)
g2 = lm(y ~ factor)
summary(g1)$coef  # we can see mean of reference equals to estimates of intercept
summary(g2)$coef
anova(g1) # Wrong result (because of df=3)
anova(g2)
summary(g1)$sigma
summary(g2)$sigma
cbind(residuals(g1), residuals(g2))
cbind(fitted(g1), fitted(g2))
tapply(y, factor, mean)
factor = relevel(factor, ref=2) # reference level=2
model.matrix(y ~ factor)
g3 = lm(y ~ factor)
summary(g2)$coef
summary(g3)$coef
anova(g2)
anova(g3)
## anova function has same result regardless of reference level
# Contrast
## one-way ANOVA is not clear which of the means differ from the others.
## constructing contrast matrix can solve this problem
## using t-test example
## estimated intercept is the mean of Group1
## factor2 is the difference between Group2 and Group1
## factor3 is the difference between Group3 and Group1
factor = gl(3, 10)
summary(lm(y ~ factor))
anova(lm(y ~ factor))
# All Pairwise Test
## By above, the difference between Group2 and 3 is not tested
## pairwise.t.test() : all possible pairwise t-tests
pairwise.t.test(y, factor, p.adjust.method="bonferroni")
pairwise.t.test(y, factor, p.adjust.method="holm")
## A popular multiple comparison test for post-hoc ANOVA is Tukey's HSD test
TukeyHSD(aov(y ~ factor))
plot(TukeyHSD(aov(y ~ factor)))
## Example : No difference between the means
set.seed(1234)
y = rnorm(30, 1.9, 1)
y
factor = gl(3, 10)
g = lm(y ~ factor)
anova(g)
pairwise.t.test(y, factor, p.adjust.method="bonferroni")
TukeyHSD(aov(g))
plot(TukeyHSD(aov(g)))
# Example of One-way ANOVA
BiocManager::install("ALL")
library(ALL)
data(ALL)
B1B2B3 = ALL$BT %in% c("B1", "B2", "B3")
ex = exprs(ALL)
y = as.numeric(ex[row.names(ex)=="1866_g_at", B1B2B3])
factor = factor(ALL$BT[B1B2B3], labels=c("B1", "B2", "B3"))
res = residuals(lm(y ~ factor))
shapiro.test(res) # reject H0 -> normality does not exist
par(mfrow=c(1, 2))
qqnorm(res, pch=19, cex.lab=1.5, col="red", main=NULL)
qqline(res)
hist(res, nclass=20, col="orange", xlab="residuals", main="") # looks like skewd normal distribution
## Breusch and Pagan Null hypothesis : error variances are all equal
install.packages("lmtest")
library(lmtest)
bptest(lm(y ~ factor), studentize=FALSE)  # reject H0 -> error variances are not equal
bartlett.test(y ~ factor)
fligner.test(y, factor)
library(car)
leveneTest(y, factor)
fit = lm(y ~ factor)$fit
par(mfrow=c(1,1))
plot(fit, res, pch=19, col="red", xlab="Fitted values", ylab="Residuals")
# Robust Tests
## When homoscedasticity is violated -> 'Welch's ANOVA'
## When normality is violated -> 'Kruskal-Wallis rank sum test'
## Example (Unequal variances)
oneway.test(y ~ factor, var.equal=FALSE)
oneway.test(y ~ factor, var.equal=TRUE)
anova(lm(y ~ factor))
summary(aov(lm(y ~ factor)))
kruskal.test(y ~ factor)
f2 = function(t) kruskal.test(t ~ factor)$p.value
kruskal.pValues = apply(ex[, B1B2B3], 1, f2)
ex[,,B1B2B3]
ex[,B1B2B3]
dim(ex[,B1B2B3])
pBF = p.adjust(kruskal.pValues, method="bonferroni")
pH0 = p.adjust(kruskal.pValues, method="holm")
pBH = p.adjust(kruskal.pValues, method="BH")
alpha = 0.05
c(sum(pBF < alpha), sum(pH0 < alpha), sum(pBH < alpha))
## Example Two-way ANOVA
library(ALL)
data(ALL)
ex = exprs(ALL)
dim(ex)
table(ALL$BT)
table(ALL$mol.biol)
w1 = ALL$BT %in% c("B", "B1", "B2", "B3", "B4")
w2 = ALL$mol.biol %in% c("BCR/ABL", "NEG")
ex12 = ex["32069_at", w1 & w2]
length(ex12)
facB = ceiling(as.integer(ALL$BT[w1 & w2])/3)
facB
ALL$BT[w1 & w2]
ALL$BT[w1 & w2]/3
as.integer(ALL$BT[w1 & w2])/3
as.integer(ALL$BT[w1 & w2])/
as.integer(ALL$BT[w1 & w2])
as.integer(ALL$BT[w1 & w2])
data.frame(B, type=ALL$BT[w1 & w2], facB=facB)
data.frame(B.type=ALL$BT[w1 & w2], facB=facB)
fac1 = factor(facB, levels=1:2, labels=c("B012", "B34"))
fac2 = factor(ALL$mol.biol[w1 & w2])
table(fac1)
table(fac2)
tapply(ex12, fac1, mean)
tapply(ex12, fac2, mean)
col = c("orange", "blue")
xlab1 = "B-cell ALL stage"
xlab2 = "Molecular biology"
ylab = "NEDD4 expression"
par(mfrow=c(1, 2))
stripchart(ex12 ~ fac1, method="jitter", cex.lab=1.5,
vertical=TRUE, xlab=xlab1, ylab=ylab, col=col)
stripchart(ex12 ~ fac2, method="jitter", cex.lab=1.5,
vertical=TRUE, xlab=xlab2, ylab=ylab, col=col)
boxplot(ex12 ~ fac1, cex.lab=1.5, main=NULL, boxwex=0.3,
xlab=xlab1, ylab=ylab, col=col)
boxplot(ex12 ~ fac2, cex.lab=1.5, main=NULL, boxwex=0.3,
xlab=xlab2, ylab=ylab, col=col)
interaction.plot(fac1, fac2, ex12, type="b", col=col, xlab=xlab1,
pch=c(16, 17), lty=1, lwd=2, legend=F, ylab=ylab)
legend("topright", c("BCR/ABL","NEG"), bty="n", lty=1, lwd=2,
pch=c(16,17), col=col, inset=0.02)
interaction.plot(fac2, fac1, ex12, type="b", col=col, xlab=xlab2,
pch=c(16, 17), lty=1, lwd=2, legend=F, ylab=ylab)
legend("topright", c("B012","B34"), bty="n", lty=1, lwd=2,
pch=c(16,17), col=col, inset=0.02)
tapply(ex12[fac2=="BCR/ABL"], fac1[fac2=="BCR/ABL"], mean)
tapply(ex12[fac2=="NEG"], fac1[fac2=="NEG"], mean)
tapply(ex12[fac1=="B012"], fac2[fac1=="B012"], mean)
tapply(ex12[fac1=="B34"], fac2[fac1=="B34"], mean)
anova(lm(ex12 ~ fac1 * fac2))
summary(lm(ex12 ~ fac1 * fac2))
## In bioinformatics, computing the number of probes with significant main as well as significant interaction effects is important
w1 = ALL$BT %in% c("B", "B1", "B2", "B3", "B4")
w2 = ALL$mol.biol %in% c("BCR/ABL", "NEG")
ex = exprs(ALL)
exw = ex[, w1 & w2]
dim(exw)
f1 = function(t) anova(lm(t ~ fac1 * fac2))$Pr[1:3]
anova.pValues = apply(exw, 1, f1)
anova.pValues.t = data.frame(t(anova.pValues))
cname = c("mainEffect1", "mainEffect2", "interaction")
colnames(anova.pValues.t) = cname
anova.pValues.t[1:10, ]
alpha = 0.05
c1 = anova.pValues.t$mainEffect1 < alpha
c2 = anova.pValues.t$mainEffect2 < alpha
c3 = anova.pValues.t$interaction < alpha
sum(c1 & c2 & c3)
alpha = 0.05/nrow(anova.pValues.t)
alpha
version()
version
updateR()
library(installr)
updateR()
install.packages("devtools")
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR(admin_password = 'Admin user password')
updateR()
updateR(admin_password="ehgus1805!")
updateR(admin_password='ehgus1805!')
updateR(admin_password=ehgus1805!)
updateR()
!]
version()
version
wd = paste(getwd(), "/Multivariate_Analysis_1/data", sep="")
setwd(wd)
### [Step 1] Data Matrix
data = read.table("trackrecord2005-women.txt", header=T)
X = data
### [Step 2] Covariance Matrix S & Correlation Matrix R
R = cor(X); S = cov(X)
S
R
### [Step 3] Spectral Decomposition
eigen.S = eigen(S) ; eigen.R = eigen(R)
eigen.S$values # Eigenvalues of S
eigen.R$values # Eigenvalues of R
V.S = eigen.S$vectors ; V.R = eigen.R$vectors # Eigenvectors of S,R
V.S
V.R
V.R[,1] = -1 * V.R[,1]
V.R
### [Step 4] Choice of Eigenvalues and Eigenvectors
gof.S = eigen.S$values/sum(eigen.S$values)*100 ; gof.R = eigen.R$values/sum(eigen.R$values)*100  # Goodness-of-fit of S,R
gof.S
gof.R
### [Step 5] PCs : linear combination of original variables
V2.S = V.S[, 1:2] ; V2.R = V.R[, 1:2]
V2.S
V2.R
### [Step 6] PCS, PCs Scores and New Data Matrix P
Z = scale(X, scale=TRUE) # Studentizied Data Matrix
Z
P.S = Z %*% V2.S ; P.R = Z %*% V2.R
### [Step 7] Plot of PCs Scores
par(mfrow=c(1,2))
plot(P.S[,1], P.S[, 2], main="Plot of PCs Scores of S", xlab="1st PC", ylab="2nd PC")
text(P.S[,1], P.S[, 2]+0.1, labels=rownames(P.S), cex=0.8, col="blue")
abline(v=0, h=0)
plot(P.R[,1], P.R[, 2], main="Plot of PCs Scores of R", xlab="1st PC", ylab="2nd PC")
text(P.R[,1], P.R[, 2]+0.1, labels=rownames(P.R), cex=0.8, col="red")
abline(v=0, h=0)
# sol2)
### 데이터의 변수마다 분산의 차이가 있으므로 공분산행렬보다 상관행렬로 해석하는것이 바람직할것이다
par(mfrow=c(1,1))
gof.R
plot(eigen.R$values, type="b", main="Scree Graph of R", xlab="Component Number", ylab="Eigenvalue")
### Correlations between PCs and variables
Ds = diag(diag(1/sqrt(R)))
D = diag(sqrt(eigen.R$values[1:2]))
corr = Ds %*% V2.R %*% D
rownames(corr) = colnames(X) ; colnames(corr) = c("comp1", "comp2")
corr
#sol_4)
ranking_comp1 = cbind(P.R[order(P.R[,1], decreasing=TRUE), 1], 1:nrow(X))
colnames(ranking_comp1) = c("PCscore", "ranking")
ranking_comp1
plot(P.R[,1], P.R[, 2], main="Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P.R[,1], P.R[, 2], labels=rownames(P.R), cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
Z = scale(X, scale=T)
# Biplot based on the Singular Value Decomposition
svd.Z = svd(Z)
U = svd.Z$u
V = svd.Z$v
D = diag(svd.Z$d)
G = (sqrt(n-1)*U)[, 1:2]
H = (sqrt(1/(n-1))*V %*% D)[, 1:2]
rownames(G) = rownames(X)
rownames(H) = colnames(X)
# Goodness-of-fit
eig = (svd.Z$d)^2
per = eig/sum(eig)*100
gof = sum(per[1:2])
round(per, 2)
round(gof, 2)
# PC Biplot
par(mfrow=c(1,1))
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(82.97%)",ylab="2nd PC(8.98%)", main="Biplot",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
