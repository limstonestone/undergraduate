plot(fit, res, pch=19, col="red", xlab="Fitted values", ylab="Residuals")
## Example (Unequal variances)
oneway.test(y ~ factor, var.equal=FALSE)
# Least Square Estimation
data(golub, package="multtest")
zyxin = grep("Zyxin", golub.gnames[,2], ignore.case=TRUE)
cmyb = grep("c-myb", golub.gnames[,2], ignore.case=TRUE)
x = golub[zyxin, ]
y = golub[cmyb, ]
leastSquares = lm(y ~ x)
leastSquares$coef
plot(x, y, pch=19, xlab="Relative zyxin gene expression", ylab="Relative c-MYB gene expression", cex.lab=1.5, col="blue")
abline(leastSquares$coef, lwd=3, lty=2, col="red")
lmSummary = summary(leastSquares)
lmSummary
## Signif.codes : more stars represents more significance
## R^2 : indicates how well the data points fit the statistical model
## (percentage of variance in the dependent variable that is explained by the model)
## adjusted R-sqaured : more predictors -> improve R-squared. adjusted R-sqaured can adjust this unreasonable issue.
# F Test Statistic
## F-value means "better than random"
## F-value is the ratio of the variance explained over the unexplained variance (residual)
f.value = (coef(lmSummary)[2, "t value"])^2 # In the simple case -> F = t^2
f.value
lmSummary$fstat
# Linear Model with a Factor
y = c(2, 3, 1, 2, 8, 7, 9, 8, 11, 12, 13, 12)
factor = gl(3, 4) # gl : Generate factors by specifying the pattern of their levels.
factor
model.matrix(y ~ factor - 1)  # model.matrix = design matrix X, -1 indicates model has no intercept
## estimation of the coefficients = estimates of the population group means from the sample data
summary(lm(y ~ factor -1))  # p-values are too small -> Reject H0 (mu_j = 0)
## linear model is useful for testing hypotheses about group means
# One-way Analysis of Variance
## ANOVA can perform for testing three or more population means are equal to each other
## SSW(sum of squares within) is some of the squared deviation of the measurements to their group mean
## SSB(sum of squares betwwen) is the sum of sqaures the deviances of the group mean w.r.t total mean
## f-value is defined by {SSB/(g-1)}/{SSW/(N-g)}
## if F-value > f => H0(mu_1 = mu_2 = mu_3 ,,, ) is rejected
y = c(2, 3, 1, 2, 8, 7, 9, 8, 11, 12, 13, 12)
factor = gl(3, 4)
groupMeans = as.numeric(tapply(y, factor, mean))  # tapply : perform function about factor
groupMeans
mean(y)
g = 3
n = 4
N = 12
ssb = 0
for (j in 1:g) {
ssb = ssb + (groupMeans[j] - mean(y))^2
}
SSB = n * ssb
SSB
SSW = 0
for (j in 1:g) {
SSW = SSW + sum((y[factor==j] - groupMeans[j])^2)
}
f.value = (SSB/(g-1)) / (SSW/(N-g))
f.value
pf(f.value, g-1, N-g, lower.tail=FALSE) # lower.tail : calculate only tail
g = lm(y ~ factor)
anova(g)
summary(aov(g))
## fator : SSB
## residuals : SSW
# Linear Model in ANOVA
set.seed(123)
y = c(rnorm(10, -1, 1), rnorm(10, 0, 1), rnorm(10, 1, 1))
y
factor = gl(3, 10)
factor
model.matrix(y ~ factor -1)
model.matrix(y ~ factor)  # Reference : factor1
g1 = lm(y ~ factor -1)
g2 = lm(y ~ factor)
summary(g1)$coef  # we can see mean of reference equals to estimates of intercept
summary(g2)$coef
anova(g1) # Wrong result (because of df=3)
anova(g2)
summary(g1)$sigma
summary(g2)$sigma
cbind(residuals(g1), residuals(g2))
cbind(fitted(g1), fitted(g2))
tapply(y, factor, mean)
factor = relevel(factor, ref=2) # reference level=2
model.matrix(y ~ factor)
g3 = lm(y ~ factor)
summary(g2)$coef
summary(g3)$coef
anova(g2)
anova(g3)
## anova function has same result regardless of reference level
# Contrast
## one-way ANOVA is not clear which of the means differ from the others.
## constructing contrast matrix can solve this problem
## using t-test example
## estimated intercept is the mean of Group1
## factor2 is the difference between Group2 and Group1
## factor3 is the difference between Group3 and Group1
factor = gl(3, 10)
summary(lm(y ~ factor))
anova(lm(y ~ factor))
# All Pairwise Test
## By above, the difference between Group2 and 3 is not tested
## pairwise.t.test() : all possible pairwise t-tests
pairwise.t.test(y, factor, p.adjust.method="bonferroni")
pairwise.t.test(y, factor, p.adjust.method="holm")
## A popular multiple comparison test for post-hoc ANOVA is Tukey's HSD test
TukeyHSD(aov(y ~ factor))
plot(TukeyHSD(aov(y ~ factor)))
## Example : No difference between the means
set.seed(1234)
y = rnorm(30, 1.9, 1)
y
factor = gl(3, 10)
g = lm(y ~ factor)
anova(g)
pairwise.t.test(y, factor, p.adjust.method="bonferroni")
TukeyHSD(aov(g))
plot(TukeyHSD(aov(g)))
# Example of One-way ANOVA
BiocManager::install("ALL")
library(ALL)
data(ALL)
B1B2B3 = ALL$BT %in% c("B1", "B2", "B3")
ex = exprs(ALL)
y = as.numeric(ex[row.names(ex)=="1866_g_at", B1B2B3])
factor = factor(ALL$BT[B1B2B3], labels=c("B1", "B2", "B3"))
res = residuals(lm(y ~ factor))
shapiro.test(res) # reject H0 -> normality does not exist
par(mfrow=c(1, 2))
qqnorm(res, pch=19, cex.lab=1.5, col="red", main=NULL)
qqline(res)
hist(res, nclass=20, col="orange", xlab="residuals", main="") # looks like skewd normal distribution
## Breusch and Pagan Null hypothesis : error variances are all equal
install.packages("lmtest")
library(lmtest)
bptest(lm(y ~ factor), studentize=FALSE)  # reject H0 -> error variances are not equal
bartlett.test(y ~ factor)
fligner.test(y, factor)
library(car)
leveneTest(y, factor)
fit = lm(y ~ factor)$fit
par(mfrow=c(1,1))
plot(fit, res, pch=19, col="red", xlab="Fitted values", ylab="Residuals")
# Robust Tests
## When homoscedasticity is violated -> 'Welch's ANOVA'
## When normality is violated -> 'Kruskal-Wallis rank sum test'
## Example (Unequal variances)
oneway.test(y ~ factor, var.equal=FALSE)
oneway.test(y ~ factor, var.equal=TRUE)
anova(lm(y ~ factor))
summary(aov(lm(y ~ factor)))
kruskal.test(y ~ factor)
f2 = function(t) kruskal.test(t ~ factor)$p.value
kruskal.pValues = apply(ex[, B1B2B3], 1, f2)
ex[,,B1B2B3]
ex[,B1B2B3]
dim(ex[,B1B2B3])
pBF = p.adjust(kruskal.pValues, method="bonferroni")
pH0 = p.adjust(kruskal.pValues, method="holm")
pBH = p.adjust(kruskal.pValues, method="BH")
alpha = 0.05
c(sum(pBF < alpha), sum(pH0 < alpha), sum(pBH < alpha))
## Example Two-way ANOVA
library(ALL)
data(ALL)
ex = exprs(ALL)
dim(ex)
table(ALL$BT)
table(ALL$mol.biol)
w1 = ALL$BT %in% c("B", "B1", "B2", "B3", "B4")
w2 = ALL$mol.biol %in% c("BCR/ABL", "NEG")
ex12 = ex["32069_at", w1 & w2]
length(ex12)
facB = ceiling(as.integer(ALL$BT[w1 & w2])/3)
facB
ALL$BT[w1 & w2]
ALL$BT[w1 & w2]/3
as.integer(ALL$BT[w1 & w2])/3
as.integer(ALL$BT[w1 & w2])/
as.integer(ALL$BT[w1 & w2])
as.integer(ALL$BT[w1 & w2])
data.frame(B, type=ALL$BT[w1 & w2], facB=facB)
data.frame(B.type=ALL$BT[w1 & w2], facB=facB)
fac1 = factor(facB, levels=1:2, labels=c("B012", "B34"))
fac2 = factor(ALL$mol.biol[w1 & w2])
table(fac1)
table(fac2)
tapply(ex12, fac1, mean)
tapply(ex12, fac2, mean)
col = c("orange", "blue")
xlab1 = "B-cell ALL stage"
xlab2 = "Molecular biology"
ylab = "NEDD4 expression"
par(mfrow=c(1, 2))
stripchart(ex12 ~ fac1, method="jitter", cex.lab=1.5,
vertical=TRUE, xlab=xlab1, ylab=ylab, col=col)
stripchart(ex12 ~ fac2, method="jitter", cex.lab=1.5,
vertical=TRUE, xlab=xlab2, ylab=ylab, col=col)
boxplot(ex12 ~ fac1, cex.lab=1.5, main=NULL, boxwex=0.3,
xlab=xlab1, ylab=ylab, col=col)
boxplot(ex12 ~ fac2, cex.lab=1.5, main=NULL, boxwex=0.3,
xlab=xlab2, ylab=ylab, col=col)
interaction.plot(fac1, fac2, ex12, type="b", col=col, xlab=xlab1,
pch=c(16, 17), lty=1, lwd=2, legend=F, ylab=ylab)
legend("topright", c("BCR/ABL","NEG"), bty="n", lty=1, lwd=2,
pch=c(16,17), col=col, inset=0.02)
interaction.plot(fac2, fac1, ex12, type="b", col=col, xlab=xlab2,
pch=c(16, 17), lty=1, lwd=2, legend=F, ylab=ylab)
legend("topright", c("B012","B34"), bty="n", lty=1, lwd=2,
pch=c(16,17), col=col, inset=0.02)
tapply(ex12[fac2=="BCR/ABL"], fac1[fac2=="BCR/ABL"], mean)
tapply(ex12[fac2=="NEG"], fac1[fac2=="NEG"], mean)
tapply(ex12[fac1=="B012"], fac2[fac1=="B012"], mean)
tapply(ex12[fac1=="B34"], fac2[fac1=="B34"], mean)
anova(lm(ex12 ~ fac1 * fac2))
summary(lm(ex12 ~ fac1 * fac2))
## In bioinformatics, computing the number of probes with significant main as well as significant interaction effects is important
w1 = ALL$BT %in% c("B", "B1", "B2", "B3", "B4")
w2 = ALL$mol.biol %in% c("BCR/ABL", "NEG")
ex = exprs(ALL)
exw = ex[, w1 & w2]
dim(exw)
f1 = function(t) anova(lm(t ~ fac1 * fac2))$Pr[1:3]
anova.pValues = apply(exw, 1, f1)
anova.pValues.t = data.frame(t(anova.pValues))
cname = c("mainEffect1", "mainEffect2", "interaction")
colnames(anova.pValues.t) = cname
anova.pValues.t[1:10, ]
alpha = 0.05
c1 = anova.pValues.t$mainEffect1 < alpha
c2 = anova.pValues.t$mainEffect2 < alpha
c3 = anova.pValues.t$interaction < alpha
sum(c1 & c2 & c3)
alpha = 0.05/nrow(anova.pValues.t)
alpha
version()
version
updateR()
library(installr)
updateR()
install.packages("devtools")
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR(admin_password = 'Admin user password')
updateR()
updateR(admin_password="ehgus1805!")
updateR(admin_password='ehgus1805!')
updateR(admin_password=ehgus1805!)
updateR()
!]
version()
version
version
wd = paste(getwd(), "/Multivariate_Analysis_1/data", sep="")
setwd(wd)
# sol1)
### [Step 1] Data Matrix
data = read.table("trackrecord2005-women.txt", header=T)
X = data
### [Step 2] Covariance Matrix S & Correlation Matrix R
S = cov(X) ;R = cor(X)
S
R
### [Step 3] Spectral Decomposition
eigen.S = eigen(S) ; eigen.R = eigen(R)
eigen.S$values # Eigenvalues of S
eigen.R$values # Eigenvalues of R
V.S = eigen.S$vectors ; V.R = eigen.R$vectors # Eigenvectors of S,R
### [Step 4] Choice of Eigenvalues and Eigenvectors
gof.S = eigen.S$values/sum(eigen.S$values)*100 ; gof.R = eigen.R$values/sum(eigen.R$values)*100  # Goodness-of-fit of S,R
gof.S
gof.R
par(mfrow=c(1,2))
plot(eigen.S$values, type="b", main="Scree Graph of S", xlab="Component Number", ylab="Eigenvalue")
plot(eigen.R$values, type="b", main="Scree Graph of R", xlab="Component Number", ylab="Eigenvalue")
### S / R 모두 첫번째 주성분만 선택하여도 충분한 설명력 가짐 (70프로 이상)
### 하지만 변수간의 분산차이가 많이 나므로 공분산행렬S 보다는 자기상관행렬R로의 해석이 정확할 것
### R의 결과로 보면, 두번째 주성분 까지 선택한다면, 설명력이 매우 높아짐. 그 뒤의 값들은 설명력이 작음
### 또한 R의 Scree plot의 elbow point가 주성분3 근처에서 이루어짐을 알 수 있음 -> 따라서 주성분의 수는 2개가 적절
### [Step 5] PCs : linear combination of original variables
V2.S = V.S[, 1:2] ; V2.R = V.R[, 1:2]
V2.S
V2.R
### [Step 6] PCS, PCs Scores and New Data Matrix P
Y = scale(X, scale=F) # Centered Data Matrix
Y
P.S = Y %*% V2.S ; P.R = Y %*% V2.R
### [Step 7] Plot of PCs Scores
par(mfrow=c(1,2))
plot(P.S[,1], P.S[, 2], main="Plot of PCs Scores of S", xlab="1st PC", ylab="2nd PC")
text(P.S[,1], P.S[, 2]+0.2, labels=rownames(P.S), cex=0.8, col="blue")
abline(v=0, h=0)
plot(P.R[,1], P.R[, 2], main="Plot of PCs Scores of R", xlab="1st PC", ylab="2nd PC")
text(P.R[,1], P.R[, 2]+0.6, labels=rownames(P.R), cex=0.8, col="red")
abline(v=0, h=0)
# sol2)
### 데이터의 변수마다 분산의 차이가 있으므로 공분산행렬보다 자기상관행렬로 해석하는것이 바람직할것이다
par(mfrow=c(1,1))
gof.R
plot(eigen.R$values, type="b", main="Scree Graph of R", xlab="Component Number", ylab="Eigenvalue")
### R의 결과로 보면, 두번째 주성분 까지 선택한다면, 설명력이 매우 높아짐(도합 90프로). 그 뒤의 값들은 설명력이 작음
### 또한 R의 Scree plot의 elbow point가 주성분3 근처에서 이루어짐을 알 수 있음 -> 따라서 주성분의 수는 2개가 적절
### Correlations between PCs and variables
Ds = diag(diag(1/sqrt(R)))
D = diag(sqrt(eigen.R$values[1:2]))
corr = Ds %*% V2.R %*% D
rownames(corr) = colnames(X) ; colnames(corr) = c("comp1", "comp2")
corr
### 주성분1을 결정하는데 가장 큰 기여를한 변수는 상관계수 0.9513832인 x800(800m(분)) 변수이다
### 주성분1을 결정하는데 기여를 가장 덜한 변수는 상관계수 0.8560043인 Marathon 기록 변수이다
### 또한 주성분1을 결정하는데에는 모든 변수가 양의 방향을 가지고 기여를 하였다
### 주성분2를 결정하는데 가장 큰 기여를한 변수는 상관계수 -0.364220인 x400(400m(초)) 변수이다
### 주성분2를 결정하는데 기여를 가장 덜한 변수는 상관계수 0.1278522인 x800(800m(분)) 변수이다
### x100, x200, x400 변수는 주성분2를 결정하는데 음의 영향을 주었고,
### x800, x1500, X3000, Marathon 변수는 주성분2를 결정하는데 양의 영향을 주었다
### x100, x200, x400의 측정 단위는 초 단위이고, x800, x1500, x3000, Marathon 변수의 측정단위는 분 단위이다
### 따라서 측정단위에서 차이가 났을 수 있음을 생각해 볼 수 있다
corr
#sol_3)
gof.R
plot(eigen.R$values, type="b", main="Scree Graph of R", xlab="Component Number", ylab="Eigenvalue")
#sol_4)
P.R
sort(P.R)
P.R[order(P.R)]
P.R[order(P.R),1]
P.R[order(P.R),]
P.R
P.R[,1]
P.R[order(P.R)]
P.R[order(P.R), ]
P.R[order(P.R[, c(1,2)]), ]
P.R[order(P.R[c(1,2)]), ]
P.R[order(P.R[,c(1,2)]), ]
P.R
P.R[order(c([1, 2]))]
P.R[order(c('1','2'))]
P.R[,order(c('1','2'))]
P.R[,order('1')]
P.R[order(P.R$1),]
P.R[order(P.R$'1'), ]
P.R[order(P.R[,1]), ]
P.R[order(P.R[,1], decreasing=TRUE), ]
P.R[order(P.R[,1], decreasing=TRUE), 1]
matrix(P.R[order(P.R[,1], decreasing=TRUE), 1])
P.R[order(P.R[,1], decreasing=TRUE), 1]
#sol_4)
ranking_comp1 = P.R[order(P.R[,1], decreasing=TRUE), 1]
rownames(ranking_comp1) = rownames(X)
ranking_comp1
#sol_4)
ranking_comp1 = cbind(P.R[order(P.R[,1], decreasing=TRUE), 1], 1:nrow(X))
ranking_comp1
colnames(ranking_comp1) = c("PCscore", "ranking")
ranking_comp1
#sol_5)
lim = range(pretty(P.R))  # pretty func computes a sequence of equally spaced round values
plot(P.R[, 1], P.R[, 2], main="Plot of PCs Scores", xlim=lim, ylim=lim,
xlab="1st PC", ylab="2nd PC")
text(P.R[,1]+2, P.R[, 2], rownames(X), cex=0.8, col="blue", pos=3)
abline(v=0, h=0)
plot(ranking_comp1)
plot(ranking_comp1["ranking"], ranking_comp1["PCscore"])
plot(ranking_comp1[, "ranking"], ranking_comp1[, "PCscore"])
text(P.R[,1]+2, P.R[, 2], rownames(X), cex=0.8, col="blue", pos=3)
text(P.R[,1]+10, P.R[, 2], rownames(X), cex=0.8, col="blue", pos=3)
text(P.R[,1]+40, P.R[, 2], rownames(X), cex=0.8, col="blue", pos=3)
text(P.R[,1]+40, P.R[, 2]+10, rownames(X), cex=0.8, col="blue", pos=3)
dev.off()
plot(ranking_comp1[, "ranking"], ranking_comp1[, "PCscore"])
#sol_5)
Z = scale(X, scale=T)
# Biplot based on the Singular Value Decomposition
svd.Z = svd(Z)
U = svd.Z$u
V = svd.Z$v
D = diag(svd.Z$d)
G = (sqrt(n-1)*U)[, 1:2]
H = (sqrt(1/(n-1))*V %*% D)[, 1:2]
rownames(G) = rownames(X)
rownames(H) = colnames(X)
# Goodness-of-fit
eig = (svd.Z$d)^2
per = eig/sum(eig)*100
gof = sum(per[1:2])
round(per, 2)
round(gof, 2)
# PC Biplot
par(mfrow=c(1,1))
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot for KLPGA Data ",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
# PC Biplot
par(mfrow=c(1,1))
dev.off()
# PC Biplot
par(mfrow=c(1,1))
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot for KLPGA Data ",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
# PC Biplot
par(mfrow=c(1,1))
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot for KLPGA Data ",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
biplot(G,H, xlab="1st PC(71.83%)",ylab="2nd PC(18.64%)", main="Biplot",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
biplot(P.R[,1], P.R[,2])
gof
per
biplot(G,H, xlab="1st PC(82.97%)",ylab="2nd PC(8.98%)", main="Biplot",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
U %*% D
P = U %*% D
U%D
U %*% D
U %*% D[,1]
sort(U %*% D[,1])
sort(U %*% D[,1], decreasing=TRUE)
Z %*% V[1]
Z %*% V[,1]
order(Z %*% V[,1])
P = Z %*% V[,1]
P
P = Z %*% V[,1]
P
P[order(P),]
P = Z %*% V[,1:2]
P[order(P),]
P = Z %*% V[,1]
P[order(P),]
P[order(P, decreasing=TRUE),]
eig
G
biplot(P.R[,1], P.R[,2])
plot(P.R[,1], P.R[, 2], main="Plot of PCs Scores of R", xlab="1st PC", ylab="2nd PC")
text(P.R[,1], P.R[, 2]+0.6, labels=rownames(P.R), cex=0.8, col="red")
source("~/Desktop/3-1/Multivariate_Analysis_1/과제2/다변량통계학(1)_과제1_201811530통계학과임도현.R", echo=TRUE)
biplot(G,H, xlab="1st PC(82.97%)",ylab="2nd PC(8.98%)", main="Biplot",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
P = Z %*% V.R[,1:2]
biplot(P.R[,1], P.R[,2])
biplot(P[,1], P[,2])
G
H
P
P = Z %*% V2.R
P
X
### 주성분행렬도를 살펴보기 위하여 비정칙값분해 수행
Z = scale(X, scale=T)
# Biplot based on the Singular Value Decomposition
svd.Z = svd(Z)
U = svd.Z$u
V = svd.Z$v
D = diag(svd.Z$d)
G = (sqrt(n-1)*U)[, 1:2]
H = (sqrt(1/(n-1))*V %*% D)[, 1:2]
rownames(G) = rownames(X)
rownames(H) = colnames(X)
# Goodness-of-fit
eig = (svd.Z$d)^2
per = eig/sum(eig)*100
gof = sum(per[1:2])
round(per, 2)
round(gof, 2)
# PC Biplot
par(mfrow=c(1,1))
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(82.97%)",ylab="2nd PC(8.98%)", main="Biplot",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
plot(P.R[,1], P.R[, 2], main="(a) Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P.R[,1], P.R[, 2], labels=rownames, cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
P.R
plot(P.R[,1], P.R[, 2], main="(a) Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P.R[,1], P.R[, 2], labels=rownames(P.R), cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
plot(P.R[,1], P.R[, 2], main="Plot of PCs Scores", xlab="1st PC", ylab="2nd PC")
text(P.R[,1], P.R[, 2], labels=rownames(P.R), cex=0.8, col="blue", pos=1)
abline(v=0, h=0)
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(82.97%)",ylab="2nd PC(8.98%)", main="Biplot",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
# PC Biplot
par(mfrow=c(1,1))
lim = range(pretty(G))
biplot(G,H, xlab="1st PC(82.97%)",ylab="2nd PC(8.98%)", main="Biplot",
xlim=lim,ylim=lim,cex=0.8,pch=16)
abline(v=0,h=0)
# sol2)
### 데이터의 변수마다 분산의 차이가 있으므로 공분산행렬보다 자기상관행렬로 해석하는것이 바람직할것이다
par(mfrow=c(1,1))
gof.R
plot(eigen.R$values, type="b", main="Scree Graph of R", xlab="Component Number", ylab="Eigenvalue")
### Correlations between PCs and variables
Ds = diag(diag(1/sqrt(R)))
D = diag(sqrt(eigen.R$values[1:2]))
corr = Ds %*% V2.R %*% D
rownames(corr) = colnames(X) ; colnames(corr) = c("comp1", "comp2")
corr
